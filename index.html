<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>The First Workshop on Natural Language Argument-Based Explanations</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Welcome</a></li>
							<li><a href="#one">Content & Topics</a></li>
							<li><a href="#five">Invited speakers</a></li>
							<li><a href="#seven">Schedule</a></li>
							<li><a href="#six">Registration</a></li>
							<li><a href="#four">Important dates & Submission Instructions</a></li>
							<li><a href="#two">Organizers</a></li>
							<li><a href="#three">Committe</a></li>
							<li><a href="#eight">Acknowledgments</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<h1>ArgNLE</h1>
							<h2>The First Workshop on Natural Language Argument-Based Explanations</h2>
							<p>Co-located with <a href="https://www.ecai2024.eu">ECAI 2024</a>. October 20th 2024, Universidad de Santiago de Compostela.</p>
							<ul class="actions">
								<li><a href="#one" class="button scrolly">Learn more</a></li>
							</ul>
						</div>
					</section>
				
				<!-- One -->
				<section id="one" class="wrapper style1 fullscreen fade-up">
					<div class="inner">
						<h2 class="major">Content & Topics</h3>
						<p>Explainability and Computational Argumentation have usually been approached as separate, independent research topics, which neglects many aspects arising from considering the interdependencies  between them. To be effective for human users, explanations are required to be formulated in natural language, possibly in an argumentative fashion. A workshop on exploring Natural language Argument-based Explanations is proposed to investigate this challenging topic, at the crossroad of these different research fields.</p>
						<p>Providing high quality explanations for AI predictions based on machine learning is a challenging and complex task. To work well it requires, among other factors: selecting a proper level of generality/specificity of the explanation; considering assumptions about the familiarity of the explanation beneficiary with the AI task under consideration; referring to specific elements that have contributed to the decision; making use of additional knowledge (e.g., metadata) which might not be part of the prediction process; selecting appropriate examples; providing evidence supporting negative hypothesis. Finally, the system needs to formulate the explanation in a clearly interpretable, and possibly convincing, way.</p>
						<p>Given these considerations, the workshop welcome contributions showing an integrated vision of Explainable AI
							(XAI), where low level characteristics of the deep learning process are combined with higher level schemas proper of the human argumentation capacity. These integrated vision
							relies on three main considerations: 
							<ul>
								<li>In neural architectures the correlation between internalstates of the network and the justification of the network classification outcome is not well studied</li>
								<li>High quality explanations are crucially based on argumentation mechanisms (e.g., provide supporting examples and rejected alternatives)</li>
								<li>In real settings, providing explanations is inherently an interactive process involving the system and the user. Accordingly, the workshop calls for cross-disciplinary contributions in three areas, i.e., deep learning, argumentation and interactivity, to support a broader and innovative view of explainable AI</li>
							</ul>
						</p>
						<p>More precisely, the workshop is intended to discuss research challenges that will allow to advance the state of the art in explainable AI. Providing explanations to support a certain conclusion has been largely studied in logic, as a fundamental characteristic of human reasoning. As a result, both theoretical and computational models of human argumentation are investigated. The recent resurgence of AI highlighted the idea that low level system behaviors not only need to be interpretable (e.g., showing those elements that most contributed to the system decision), but also need to fit high level human schemas to produce convincing arguments.</p>
						<p>
							Suggested topics of the workshop include but are not limited to:
							<ul>
								<li>Natural language argument-based explanations</li>
								<li>Neuro-symbolic explainable argumentation</li>
								<li>Dialectical, dialogical and conversational explanations</li>
								<li>AI methods to support argumentative explainability</li>
								<li>User-acceptance and evaluation of argumentation-based explanations</li>
								<li>Tools that provide argumentation-based explanations</li>
								<li>Use of argument-based explanations for research from the social sciences, digital humanities, and related fields</li>
								<li>Real-world applications, including argument-based explanations search, customer reviews, argument analysis in meetings, and applications in specific domains, such as education, law, and scientific writing</li>
							</ul>
						</p>
					</div>
				</section>

				<!-- FIVE -->
					<section id="five" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Invited speakers</h2>
							<div class="inner">
								<ul>
									<img src="https://www.imperial.ac.uk/media/migration/faculty-of-engineering/180906_hs_francesca_toni_02--tojpeg_1560865649713_x2.jpg" alt="Francesca Toni">
									<li><a href="https://www.imperial.ac.uk/people/f.toni">Prof. Francesca Toni</a> - Faculty of Engineering, Department of Computing, Imperial College London, UK</li>
										<ul>
											<li>Title: Argumentative Explanations for Veracity-Checking</li>
											<li>Abstract: AI has become pervasive in recent years, and the need for explainability is widely agreed upon as crucial towards safe and trustworthy deployment of AI systems, especially given the plethora of opportunities for misinformation, hallucinations and malicious behaviour in data-driven AI. In this talk I will overview approaches based on computational argumentation for explaining veracity-checking in a number of incarnations, including  for fact checking, for detecting scientific fraud, and for claim verification. I will advocate computational argumentation as  ideally suited to support explainable veracity  checking that can (1) interact to progressively explain outputs and/or reasoning as well as assess grounds for contestation provided by humans and/or other machines, and (2) revise decision-making processes to redress any issues successfully raised during contestation.</li>
											<li>BIO: Francesca Toni is Professor in Computational Logic and Royal Academy of Engineering/JP Morgan Research Chair on Argumentation-based Interactive Explainable AI (XAI) at the Department of Computing, Imperial College London, UK, as well as the founder and leader of the CLArg (Computational Logic and Argumentation) research group and of the Faculty of Engineering XAI Research Centre. She holds an ERC Advanced grant on Argumentation-based Deep Interactive eXplanations (ADIX).  Her research interests lie within the broad area of  Explainable AI, at the intersection of Knowledge Representation and Reasoning, Machine Learning, Computational Argumentation, Argument Mining, and Multi-Agent Systems.  She is EurAI fellow, IJCAI Trustee, in the Board of Directors for KR Inc., member of the editorial board for the Argument and Computation journal,  Editorial Advisor for Theory and Practice of Logic Programming, and associate editor for  the AI journal.</li>
										</ul>
								</ul>
							</div>
						</div>
					</section>
					<section id="seven" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Schedule</h2>
							<ul>
								<li>14:00-14:05 Welcome</li>
								<li>14:05-15:00 Invited talk of Francesca Toni</li>
								<li>15:00-15:30 Felix Liedeker, Olivia Sanchez-Graillet, Philipp Cimiano, JÃ¶rg Wellmer, Moana Seidler and Christian Brandt. A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support</li>
								<li>15:30-16:15 Coffee break with posters on the CHIST-ERA ANTIDOTE project</li>
								<li>16:15-16:45 Chris Reed. Argument, Explanation and Inference: A position paper comprising footnotes to Hahn & Tesic</li>
								<li>16:45-17:30 Discussion panel and closing remarks</li>
						</div>
					</section>
				<!-- SIX -->
					<section id="six" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Registration</h2>
							<div class="inner">
								<p>Participants can register through <a href="https://www.ecai2024.eu/registration">this link</a></p>
							</div>
						</div>
					</section>
				
				<!-- Three -->
				<section id="four" class="wrapper style1 fade-up">
					<div class="inner">
						<div>
							<h2>Important dates</h2>
							<ul>
								<li style="color:Tomato;">Submission deadline: EXTENDED to June 30th 2024</li>
								<li>Notification of acceptance: July 19th 2024</li>
								<li>Camera-ready papers: July 31st 2024</li>
								<li>Workshop: October 20th 2024</li>
							</ul>
							<h2>Submission instructions</h2>
							<ul>
								<p>Papers must be written in English, be prepared for double-blind review using the <a href="https://ecai2024.eu/download/ecai-template.zip">ECAI LaTeX template</a>, and not exceed 7 pages (not including references). Papers should be submitted via <a href="https://easychair.org/conferences/?conf=argnle2024">EasyChair</a></p>
							</ul>
						</div>
					</div>
				</section>

				<!-- Two -->
					<section id="two" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Organizers</h2>
							<div class="inner">
								<ul>
									<li><a href="https://ragerri.github.io/">Rodrigo Agerri</a> - HiTZ Center - Ixa, University of the Basque Country UPV/EHU, Spain</li>
									<li><a href="https://www-sop.inria.fr/members/Elena.Cabrio/">Elena Cabrio</a> - UniversitÃ© CÃ´te dâAzur, Inria, CNRS, I3S, France</li>
									<li><a href="https://webusers.i3s.unice.fr/~villata/Home.html">Serena Villata</a> - UniversitÃ© CÃ´te dâAzur, Inria, CNRS, I3S, France</li>
									<li><a href="https://ifilnova.pt/en/people/marcin-lewinski/">Marcin Lewinski</a> - IFILNOVA, Universidade Nova de Lisboa, Portugal</li>
									<li><a href="http://hlt.fbk.eu/people/magnini">Bernardo Magnini</a> - Fondazione Bruno Kessler, Italy</li>
									<li><a href="https://people.cs.kuleuven.be/~sien.moens/">Marie-Francine Moens</a> - KU Leuven, Belgium</li>
								</ul>
							</div>
						</div>
					</section>

				<!-- Three -->
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Committee</h2>
							<ul>
								<li>Aitziber Atutxa - HiTZ Center - Ixa, University of the Basque Country UPV/EHU, Spain</li>
								<li>Maite Oronoz - HiTZ Center - Ixa, University of the Basque Country UPV/EHU, Spain</li>
								<li>German Rigau - HiTZ Center - Ixa, University of the Basque Country UPV/EHU, Spain</li>
								<li>Petar BodloviÄ - IFILNOVA, Universidade Nova de Lisboa, Portugal</li>
								<li>Fabrizio Macagno - IFILNOVA, Universidade Nova de Lisboa, Portugal</li>
								<li>Maria Grazia Rossi - IFILNOVA, Universidade Nova de Lisboa, Portugal</li>
								<li>Victor David - UniversitÃ© CÃ´te d'Azur, Inria, CNRS, 13S, France</li>
								<li>Benjamin Molinet - UniversitÃ© CÃ´te dâAzur, Inria, CNRS, I3S, France</li>
								<li>Theo Alkibiades Collias - UniversitÃ© CÃ´te dâAzur, Inria, CNRS, I3S, France</li>
								<li>Alberto Lavelli - Fondazione Bruno Kessler, Italy</li>
								<li>Andrea Zaninello - Fondazione Bruno Kessler, Italy</li>
								<li>Kanimozhi Uma - KU Leuven, Belgium</li>
								<li>Wei Sun - KU Leuven, Belgium</li>

						</div>
					</section>

				<!-- Eight -->
				<section id="eight" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Acknowledgments</h2>
							<div class="inner">
								<ul>
									<img src="https://i.imgur.com/PC4CBtG.png" alt="Sponsors">
										<p>This work has been partially supported by the French government, through the 3IA CÃ´te d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR- 19-P3IA-0002.
										This work was also partially supported by the CHIST-ERA grant of the Call XAI 2019 of the ANR with the grant number Project-ANR-21-CHR4-0002.</p>
										<p>We also acknowledge the support of the following MCIN/AEI/10.13039/501100011033 projects: 
											<li>Antidote (PCI2020-120717-2) and by European Union NextGenerationEU/PRTR.</li> 
											<li>DeepKnowledge (PID2021-127777OB-C21) and ERDF A way of making Europe; (iii) DeepMinor (CNS2023-144375) and European Union NextGenerationEU/PRTR.</li>
										</p>
								</ul>
							</div>
						</div>
					</section>
				</div>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="https://github.com/alki22">Theo Alkibiades Collias</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
